[["index.html", "Data Analytics &amp; Engineering Team Manual Chapter 1 Scope", " Data Analytics &amp; Engineering Team Manual Nikita Voevodin: Senior Data Engineer 2021-12-07 Chapter 1 Scope This work is licensed under a Creative Commons Attribution 4.0 International License. The New York City Taxi and Limousine Commission (TLC) is the City agency responsible for regulating for-hire transportation in New York City, including taxis, street hail liveries, high-volume for-hire services such as Uber and Lyft, black cars, luxury limousines, livery vehicles, commuter vans, and paratransit vehicles. The TLC licenses about 175,000 drivers, 115,000 cars, and 1,000 businesses, which together transport more than a million passengers a day, making TLC the most active for-hire transportation regulatory agency globally with oversight of a critical component of the Citys transportation network. With the introduction of new apps and technologies, TLC is on the front lines of a rapidly changing mobility landscape, and our innovative effortswhether regulating driver pay, ensuring wheelchair accessibility, working to eliminate traffic fatalities, or preventing discriminatory serviceoften serve as a model for other cities. The purpose of this document is to document practices, procedures, and processes of analysis and work at the Taxi and Limousine Commission (TLC) with a focus on the Analytics Unit. This will be an evolving document to be shared with current and incoming staff to maintain and capture all knowledge relevant to completing work at the TLC. In addition to laying the groundwork for standards, it will serve as a living document of the vision and strategy employed by the analytics team to help the constituency with data-driven decisions and support for policy research. The idea for the analytics team is to create a highly effective rapid-prototyping research element within TLC that will serve to do the following (examples provided below each point): Provide policy research support Medallion Relief Program Black Car and Livery Task Force and Report Regulatory Review Battery Electric Vehicle Pilot Program Driver income study analytics support Vehicle retirement adjustment Maintain automated metrics and KPIs for rapid access internally and externally Respond to internal and external data requests Open data support Rapid prototype applications and analytical processes Testing new technologies for integration with IT &amp; the taxi industry Maintaing and imporving existing solutions like TLC Data Hub Creating new tools for internal and external users Modernize data infrastructure Working towards speeding up data processes with technologies like SQL Server Datawarehousing/Apache Spark Engage with the public Publish innovative research on For-Hire industry Partnering with academia and think thanks This work is licensed under a Creative Commons Attribution 4.0 International License. "],["the-team.html", "Chapter 2 The Team", " Chapter 2 The Team The analytics team in its current form consists of the Senior Data Engineer (Unit Head), a Data Engineer, a Data Analyst, and a College Aide. The teams primary focus is: Production of KPIs &amp; metrics relevant to the industry Rapid prototyping of algorithms and analytical tools Data exploration and analysis The chain of command is as follows: AC Data &amp; Tech -&gt; Sr.Data Engineer -&gt; Data Engineer, Data Analyst, College Aide TBD -&gt; Nikita Voevodin -&gt; TBD, TBD, Phillip Wong The TBD will be in charge when the Sr.Â Data Engineer is not present. All employees ultimately answer to the Assistant Commissioner of Data and technology. The current team members are: Table 2.1: Team Name Title TBD Assistant Commissioner of Data &amp; Technology Nikita Voevodin Sr.Data Engineer TBD Data Engineer TBD Data Analyst TBD Data Analyst Phillip Wong College Aide This work is licensed under a Creative Commons Attribution 4.0 International License. "],["workflow.html", "Chapter 3 Workflow 3.1 Establishing a Data Science Environment 3.2 Professional Development 3.3 Metrics, Analytics &amp; Automation 3.4 Dashboards &amp; Apps 3.5 Passwords, Usernames, Accounts", " Chapter 3 Workflow 3.1 Establishing a Data Science Environment Each analyst operates in their own way; however, the following setups should be followed to leverage collaboration across the team: Install R, Rstudio &amp; adjacent tools R Base Rstudio Install Python 3.xxx Anaconda Set proxies SQL Server Management Studio ArcGIS or QGIS (if needed) Git NPM 3.2 Professional Development Each analyst has tools available to them for improving their data skillset: DataCamp TLC maintains a subscription to the datacamp for R, Python and more; for membership access speak to your supervisor FreeCodeCamp An excellent free course for learning html, css and javascript StackOverflow Is your friend if you are stuck YouTube Most things you can learn there R, Not the best practices 90% of the R code related stuff that you will be doing at TLC is covered there 3.3 Metrics, Analytics &amp; Automation Important: Moving forward we will focus on automating our work as best as possible. We already do for the most part. The general process: Create a script -&gt; Output a result -&gt; Automate the script to run on schedule -&gt; document (the what, the how, the when, and the why) Note: Automation is a judgment call and should be considered the default. Note that in the documentation if things are ad hoc or too custom to automate. The rule of thumb is: automate when there is a temporal element since this generally means someone will ask for this again. All final reports and analyses should go under the directory below unless otherwise directed: I:/COF/COF/DA&amp;E/your_name This folder holds other subdirectories with various output files like images, excel documents, and other relevant data. The general structure analysts on the analytics team should follow are: Unless otherwise directed, create a folder in your folder and label it with topic words of the project you are working on In that root folder, insert your main script, cache any data files in subdirectories, and provide a documentation file. Below is an example: Folder Example Note that in the folder, we have output files, the main script, and an HTML report meant to help present the work. The automate script leverages the taskcheduleR package in R to make the script run every month; this way, utilization rates for medallions and shls are updated automatically. 3.4 Dashboards &amp; Apps Currently, we are working on standardizing our analyses as best as possible to allow for quick servicing of both routine and some ad-hoc requests. As tools become more streamlined, we will be able to expand with internal dashboarding tools; currently, limitation in licensing tools and opensource software acceptance has hindered our ability to use these. Tools which we now use for dashboarding are: Dashboards Shiny R PowerBI Apps Shiny R ReactJS React Native 3.5 Passwords, Usernames, Accounts This sections has the accounts and passwords common to the whole team. TBD This work is licensed under a Creative Commons Attribution 4.0 International License. "],["data-connections-access.html", "Chapter 4 Data Connections &amp; Access 4.1 SQL Server 4.2 Pertinent IT Databases 4.3 Open Data Database 4.4 Open Data 4.5 Proxy Settings", " Chapter 4 Data Connections &amp; Access 4.1 SQL Server Our primary data source is the SQL server, which holds many databases, each with its own sets of tables accessible through different tools. Currently, the teams lean heavily on 3 of the following tools for accessing data from the server: SQL Server Management Studio R Python 4.2 Pertinent IT Databases There are a series of databases that we use to collect data. Your Windows profile restricts access to these servers, and a ticket with IT is required to access the database, but talk to your supervisor first to see if there are accessible points already in place before approaching IT. below are some credentials provided for a few databases: Table 4.1: ODBC Connections Database Driver Server Credentials DataWarehouse odbc driver 17 for sql server TLCBDWH request from IT Azure_Trip_Data odbc driver 17 for sql server tlcsqlmi01.fa986d691ca7.database.windows.net request from IT APPLUS sql server 10.224.244.114 request from IT TLC_Policy_Programs_Dev odbc driver 17 for sql server msdwvd-tlctxy01.csc.nycnet request from IT 4.3 Open Data Database Before submitting data to the city Open Data portal, we store it in the Open Data SQL server. Contact Nikita Voevodin or Maxim Smolyaninov from IT if you ever need access to it. 4.4 Open Data City agencies that work with data are often required to post their data to the citys open data portal. It can be handy to know how to pull data from there. Even for our data, different departments publish different datasets, which might not be shared between departments. You can either download the whole datasets as CSV, JSON, or whatever, or you can use their API. They have helpful code snippets of working with their data in different programming languages. The first thing you should do is set up an account and get an API key. Then, you can do something like: R: library(RSocrata) date &lt;- Sys.Date() test &lt;- read.socrata( paste0(&quot;https://data.cityofnewyork.us/resource/rhe8-mgbb.json?last_updated_date=&quot;,date,&quot;&quot;), app_token = &quot;yourtokenhere&quot;, email = &quot;yourcreds&quot;, password = &quot;yourcreds&quot; ) Python: from sodapy import Socrata; import pandas as pd; client = Socrata(&quot;data.cityofnewyork.us&quot;, &quot;token&quot;, username=&quot;yourcreds&quot;, password=&quot;yourcreds&quot;) today = date.today() results = client.get(&quot;rhe8-mgbb&quot;, limit = 20000, last_updated_date=today) # Convert to pandas DataFrame results_df = pd.DataFrame.from_records(results) 4.5 Proxy Settings At some point, you might encounter a proxy problem. That is a firewall blocking certain connections from outside. Unfortunately, everything that has to do with installing python or javascript packages is considered undesirable by our firewall and ultimately will be blocked. That is a very annoying issue to deal with for anybody. Fortunately, there is a solution. For most tasks, adding the following proxy settings will fix the problem. Open Anaconda -&gt; file -&gt; preferences -&gt; configure Conda. Paste the following in there (use your username): channels: - defaults proxy_servers: http: http://csc\\yourusername@10.155.126.15:8080 https: http://csc\\yourusername@10.155.126.15:8080 ssl_verify: false Proxy Settings Here are the settings for other languages and systems: Linux #Use the following syntax to configure the proxy for http, https and ftp traffic on the Linux #command line : # export http_proxy=&quot;http://bcpxy.nycnet:8080&quot; # export https_proxy=&quot;https://bcpxy.nycnet:8080&quot; # export ftp_proxy=&quot;http://bcpxy.nycnet:8080&quot; ------------------------------------------------------------------------------- #Use the following syntax if the proxy server requires authentication : # export http_proxy=&quot;http://user:password@bcpxy.nycnet:8080&quot; # export https_proxy=&quot;https://user:password@bcpxy.nycnet:8080&quot; # export ftp_proxy=&quot;http://user:password@bcpxy.nycnet:8080&quot; ------------------------------------------------------------------------------- #Using your email address username%40agency.nyc.gov # export http_proxy=&quot;http://username%40agency.nyc.gov:&lt;password&gt;@bcpxy.nycnet:8080&quot; # export https_proxy=https://username%40agency.nyc.gov:&lt;password&gt;@bcpxy.nycnet:8080 # export ftp_proxy=http://username%40agency.nyc.gov:&lt;password&gt;@bcpxy.nycnet:8080 NPM # npm config set proxy http://bcpxy.nycnet:8080 # npm config set https-proxy http://bcpxy.nycnet:8080 If none of this helps, contact either Nikita Voevodin or Jordan Mamet from IT. This work is licensed under a Creative Commons Attribution 4.0 International License. "],["tasks-checkups.html", "Chapter 5 Tasks &amp; CheckUps 5.1 Trello 5.2 Weekly Meetings 5.3 Evaluations", " Chapter 5 Tasks &amp; CheckUps There are four types of tasks: Major project where the whole team works together These tasks usually have scopes, timelines and are well documented and articulated. Pull requests and ad-hoc things Your supervisor would usually assign and oversee those. They are usually data requests from the leadership, other departments, or outside of the agency. They are usually less documented, and deadlines depend on their origin. Requests that went around your supervisor and straight to you If it is from a supervisor of your supervisor, just do it. If it is from other departments, prioritize it based on your availability and the nature of the request. If it is from outside of the agency, definitely notify your supervisor. In most cases, though, it is up to you as long as the priority tasks are done. Your initiatives You are encouraged to have initiatives of your own. Especially if they can benefit the agency and your growth. Most of the valuable agency projects start as small initiatives. Your supervisor will support your initiative in 99% of cases. We have a Trackit system in place. Some other departments will require you to officially submit your requests through that. You can do that too, but I would advice against that as it creates an impressions that you are unwilling to help or cooperate. It is your call however. 5.1 Trello There are many ways to track your projects, and you are free to use what works for you. I will stick with Trello task management software to assign and track tasks. It is free and convenient. Give it a shot. Trello ex 5.2 Weekly Meetings Your supervisor will place a weekly meeting on your calendar. During that (informal) meeting, you can update them on how things are going, your ideas, concerns, etc. You can reach out to your supervisor for guidance at any point on any other day. 5.3 Evaluations TBD This work is licensed under a Creative Commons Attribution 4.0 International License. "],["other-teams-contacts.html", "Chapter 6 Other Teams &amp; Contacts 6.1 Beaver 6.2 LIC (Licensing and Court) 6.3 S&amp;E (Safety and Emissions, TLC Police)", " Chapter 6 Other Teams &amp; Contacts The TLC licenses about 175,000 drivers, 115,000 vehicles, and 1,000 businesses, which together transport more than a million passengers a day, making TLC the most active for-hire transportation regulatory agency globally with oversight of a critical component of the Citys transportation network. To do all that, we need people and space. We have three main offices in NYC. Below, I would like to list the teams and contacts relevant to our work here. It might not be complete as I do not know everybody at TLC, so if you are reading this and thinking that you should be here, email me at voevodinn with relevant info. Beaver Street (Head Office) LIC (Licensing and Court) Queens (Inspection and Enforcement) 6.1 Beaver TLC head office. Most of the decision-making happens here. TLC Commissioners office, IT, HR, PR, Legal, Policy, Education, External Affairs, Programs, and other admin staff are all located at Beaver. Here are some valuable contacts for you (always in the works): 6.1.1 Data Analytics &amp; Engineering Nikita Voevodin Senior Data Engineer (Unit Head) email voevodinn@ ext 1195 ask me about: trip data, our tech, programming, data projects 6.1.2 Policy James DiGiovanni Executive Director email digiovannij@ ext  ask me about:  Ted Metz Policy Analyst email metzt@ ext  ask me about:  6.1.3 Programs 6.1.4 IT (DATA) 6.1.5 Legal 6.1.6 PR 6.1.7 External Affairs 6.1.8 HR 6.2 LIC (Licensing and Court) This office processes driver. Our prosecution and other analytics teams are located there. They deal with many interesting questions and data and hold a lot of institutional knowledge. 6.2.1 Data Analytics Adrian Chamorro Data Engineer email chamorroa@ ext  ask me about:  6.2.2 Prosecution Serge Router Prosecution Data Support Unit email Routers@ ext  ask me about:  6.3 S&amp;E (Safety and Emissions, TLC Police) This office inspects and processes vehicles. Our police and other analytics teams are located there. Inspection data is super helpful, and we at Beave have little knowledge of what goes into it. 6.3.1 Data Analytics This work is licensed under a Creative Commons Attribution 4.0 International License. "],["most-used-tables.html", "Chapter 7 Most Used Tables 7.1 Traditional Data 7.2 TLC Datawarehouse 7.3 Tables", " Chapter 7 Most Used Tables 7.1 Traditional Data Within the databases that we covered in one of the previous sections is a litany of tables that we access for various purposes. Below is a list of the popular tables we reference with a short description of what they are for. If you have any questions, talk to your supervisor. FHVHV_TripRecord Trip record table for High Volume (UBER, Lyft, VIA prior to Sep 2021) trips after 2019-01, each row represents a trip. Database: Azure_Trip_Data Note: Dont ever pull the whole table, it will crash your PC. Use datetimeid for dates - it is indexed. Sample pull: SELECT top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[FHVHV_TripRecord] where datetimeid &gt;= 2021080100 and datetimeid &lt; 2021110100 FHV_Prd_TripRecord Trip record table for all fhvs (high volume and non high volume before 2019-02) and just traditional fhvs (after 2019-01). Each row represents a trip. Database: Azure_Trip_Data Note: Dont ever pull the whole table, it will crash your PC. Use datetimeid for dates - it is indexed. Sample pull: SELECT top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[FHV_Prd_TripRecord] where datetimeid &gt;= 2021080100 and datetimeid &lt; 2021110100 vw_FHVALL_Triprecord Combined view of the 2 tables above combined. Each row represents a trip. Database: Azure_Trip_Data Note: Not every column that is present in the FHVHV_TripRecord present in the FHV_Prd_TripRecord. For example, anything that has to do with financial information. Sample pull: SELECT top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[vw_FHVALL_Triprecord] where datetimeid &gt;= 2021080100 and datetimeid &lt; 2021110100 Tpep2_triprecord Trip record table for medallion (yellow) trips after 2010, each row represents a yellow cab trip . Database: Azure_Trip_Data Note: Dont ever pull the whole table, it will crash your PC. Use datetimeid for dates - it is indexed. Sample pull: SELECT top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[Tpep2_triprecord] where datetimeid &gt;= 2021080100 and datetimeid &lt; 2021110100 Lpep2_triprecord Trip record table for SHL (green) trips after 2010, each row represents a yellow cab trip . Database: Azure_Trip_Data Note: This tables tructure is very similar to Tpep2_triprecord, but it has much fewer records. Use datetimeid for dates - it is indexed. Sample pull: SELECT top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[Lpep2_triprecord] where datetimeid &gt;= 2021080100 and datetimeid &lt; 2021110100 DimLocation Super important table if you are doing spacial analysis. Database: Azure_Trip_Data Note: Useful when you are joining it to the trip records by the locationid column Sample pull: SELECT * FROM [TPEP_AZURE].[TPEPDW].[dbo].[DimLocation] Fhv_base_list The list of bases. Bases are companies that dispatch trips. The table might not be super helpful on its own, but it is super useful when you join it to trip tables to figure out an industry or a company name of a base that dispatched a trip Database: Azure_Trip_Data Note: Not a big table. In the example below, look at the last 5 columns. Sample pull: select top 100 * FROM [TPEP_AZURE].[TPEPDW].[dbo].[FHV_Prd_TripRecord] AS TRIPS INNER JOIN [TPEP_AZURE].[TPEPDW].[dbo].[fhv_base_list] bases on TRIPS.[Dispatching_base_num] = bases.[LIC_NO] Tlc_camis_entity A snapshot of all entities (all licensees including but not limited to drivers, vehicles, bases) and pertinent information like the license application date, addresses, etc. Database: DataWarehouse Note: Very important table. We have a very extensive printed documentation for it. Ask your supervisor for it. Sample pull: All active drivers SELECT entity_nam, rtrim(ltrim(fed_id)) as fed_id, rtrim(ltrim(lic_no)) as lic_no, lic_code, lic_exp_date from tlc_camis_entity where lic_code in (&#39;HDR&#39;,&#39;CDR&#39;) and STAT_ENTITY_LIC IN (&#39;002&#39;,&#39;009&#39;,&#39;010&#39;,&#39;RNA&#39;,&#39;ANL&#39;) Tlc_plate A table holding all current and historical plate information for vehicles. Database: DataWarehouse Note: . Sample pull: pull top 100 SELECT top 100 * from tlc_plate 7.2 TLC Datawarehouse The Data Team, collaborating with IT, built out a data warehouse that automatically aggregates the most often requested data points on a set schedule, drastically increasing the speed with which data can be pulled and analyzed. Most tables in the warehouse update automatically and run on a set schedule. The standard workflow of creating a new table is as follows: Create a new table with some initial data straight from the SSMS, Python, or R. Create a Stored Procedure script. The goal of that script is to update the table that you created. Create a Job in the SSMS job scheduler. That job will run the Stored Procedure you created in step 2 on a specified schedule. Datawarehouse SSMS view: Datawarehouse ex Here is a connection example using R and Python. Note: you must have the ODBC connection set up, as shown in section 4.2 of this manual. R: library(RODBC) tp2 = odbcConnect(&quot;TLC_Policy_Programs_Dev&quot;, uid = &quot;...&quot;) test &lt;- sqlQuery(tp2, &quot;SELECT * FROM [TLC_Policy_Programs_Dev].[dbo].[high_volume_indicators_weekly_financials]&quot;) Python: import pyodbc params= urllib.parse.quote_plus(&quot;DRIVER={SQL Server};SERVER=msdwvd-tlctxy01.csc.nycnet;DATABASE=TLC_Policy_Programs_Dev;Trusted_Connection=yes&quot;) engine = create_engine(&quot;mssql+pyodbc:///?odbc_connect=%s&quot; % params) sql = &#39;&#39;&#39; SELECT * FROM [TLC_Policy_Programs_Dev].[dbo].[high_volume_indicators_weekly_financials] &#39;&#39;&#39; test = pd.read_sql_query(sql, engine) The data dictionaries for the majority of tables in the Datawarehouse are in here: I:\\COF\\COF\\_M3trics2\\automation\\data_dictionaries There is also a standardization guide for creating tables and views in the warehouse. You can access it here: I:\\COF\\COF\\_DA&amp;E_\\Nikita\\Supporting_docs 7.3 Tables There are many valuable tables in the Datawarehouse. I recommend going through the documentation folder to get accustomed to some of them. I want to list the top 5 most used tables in this document, though: industry_indicators_daily_trips This table goes back to 2014 (inclusive) for yellow and green and 2015 for fhvs. It contains trip counts aggregated by day, split by every industry. Database: TLC_Policy_Programs_Dev Note: This table will save you a ton of time. Sample pull: SELECT TOP (1000) [period_start] ,[period_end] ,[metric_day] ,[industry] ,[count_trips] FROM [TLC_Policy_Programs_Dev].[dbo].[industry_indicators_daily_trips] data_reports_monthly_indicators_all These are a set of published metrics updated every month and reviewed with the commissioner before updating. They cover a myriad of relevant metrics for specific industries we regulate. Database: TLC_Policy_Programs_Dev Note: Serves as a base for the monthly indicators that we publish to our website. Sample pull: SELECT TOP (1000) [Month_Year] ,[License_Class] ,[Trips_Per_Day] ,[Farebox_Per_Day] ,[Unique_Drivers] ,[Unique_Vehicles] ,[Vehicles_Per_Day] ,[Avg_Days_Vehicles_on_Road] ,[Avg_Hours_Per_Day_Per_Vehicle] ,[Avg_Days_Drivers_on_Road] ,[Avg_Hours_Per_Day_Per_Driver] ,[Avg_Minutes_Per_Trip] ,[Percent_of_Trips_Paid_with_Credit_Card] ,[Trips_Per_Day_Shared] FROM [TLC_Policy_Programs_Dev].[dbo].[data_reports_monthly_indicators_all] high_volume_indicators_weekly_financials These are a set of metrics we created to track driver income on a Monday to Sunday weekly schedule. Database: TLC_Policy_Programs_Dev Note: Created in a python script they piggyback off utilization to come up with our best estimate on high volume driver income. Sample pull: SELECT TOP (1000) [date] ,[metric_week] ,[aggregate_pay] ,[aggregate_hours] ,[aggregate_hourly_pay] ,[median_total_pay] ,[median_logon_hours] ,[median_hourly_pay] ,[driver_count] ,[pay_per_driver] FROM [TLC_Policy_Programs_Dev].[dbo].[high_volume_indicators_weekly_financials] order by metric_week desc industry_zone_indicators_monthly_pickups Count of pickups split by month, industry, and taxi zone (265). Database: TLC_Policy_Programs_Dev Note: This table will save you a ton of time. Sample pull: SELECT TOP (1000) [period_start] ,[period_end] ,[metric_month] ,[industry] ,[zone] ,[count_pickups] ,[count_pickups_shared] ,[count_pickups_ehail] FROM [TLC_Policy_Programs_Dev].[dbo].[industry_zone_indicators_monthly_pickups] order by [metric_month] desc company_indicators_weekly_utilization_even Driver utilization is calculated and loaded into our policy dev server. It is currently run unweighted, meaning that app logon time which is the denominator in this calculation is evenly split for apps a driver is logged into simultaneously. Database: TLC_Policy_Programs_Dev Note: Note that every nth time a year we re-evaluate utilization publicly as per the law  legal can provide more assistance on the timeline as Ryan wrote the rules. Sample pull: SELECT TOP (1000) [period_start] ,[period_end] ,[metric_week] ,[company] ,[sum_cruising_seconds] ,[sum_passenger_seconds] ,[pct_utilization] FROM [TLC_Policy_Programs_Dev].[dbo].[company_indicators_weekly_utilization_even] This section will be developed more in the future. This work is licensed under a Creative Commons Attribution 4.0 International License. "],["major-projects.html", "Chapter 8 Major Projects 8.1 TLC Datawarehouse 8.2 Data Reports - Monthly Indicators 8.3 Driver Utilization data 8.4 TLC Data Hub 8.5 Raw Trip Records publishing 8.6 Data Tasks Spreadsheet", " Chapter 8 Major Projects The data team is responsible for a lot of interesting and essential projects. Some of these projects are very important to the agencys day-to-day operations. Here is an overview of the top 5 most essential data projects. This section is flexible and will be either expanded or shrunk as needed. 8.1 TLC Datawarehouse What Known as our policy dev server, IT provides a database where we store aggregated tables and essential metrics. Every 1-2 weeks, numbers are updated to support policy. When Ongoing project. Most of the tables in the database are updated automatically following different schedules. Where #Directories and sources I:\\COF\\COF\\_M3trics2\\automation #Catalog and data dictionaries here Data science reference: I:\\COF\\COF\\_M3trics2\\automation\\data_dictionaries Who Built by the policy analytics team, it is maintained now by Nikita Voevodin with IT support from Maxim Smolyaninov. Other New tables should be created for requests that are deemed repetitive and automatable. 8.2 Data Reports - Monthly Indicators What This report is published on open data and our website and is reviewed with the commissioner every month. It includes a lot of relevant data from the traditional FHV bases (trip patterns, vehicle and driver counts, etc.) The table published on open data with the following columns: Base License Number, Base Name, DBA, Year, Month, Month Name, Total Dispatched Trips, Total Dispatched Shared Trips, Unique Dispatched Vehicles. When It is a recurring project. It is updated on the last Monday of each month. Where https://data.cityofnewyork.us/Transportation/FHV-Base-Aggregate-Report/2v9c-2k7f Who Point: Nikita Voevodin runs the updates. Support: IT/ Web Konstantin Onishchenko, PR Alan Fromberg, Rebecca Harshbarger Other N/A 8.3 Driver Utilization data What Driver utilization is calculated and loaded into our policy data warehouse. It is currently run unweighted, meaning that app logon time which is the denominator in this calculation, is evenly split for apps a driver is logged into simultaneously. Note that every nth time a year, we re-evaluate utilization publicly as per the law  legal can provide more assistance on the timeline as Ryan wrote the rules. When It is a recurring project. It is updated during the last week of each month. Where #Metrics I:\\COF\\COF\\_M3trics2\\automation\\data_dictionaries. File: &quot;company_indicators_weekly_utilization_even&quot; Who Nikita Other N/A 8.4 TLC Data Hub What TLC Data Hub offers users a new and convenient location to access and visualize taxi and for-hire industry data. TLC Data Hub uses public data available on Open Data and the TLC website and does not use, track or display any private information of the drivers or companies. The Hub currently consists of two dashboards. The Trip Viz dashboard allows the public to run queries on TLC-collected trip data, while the Industry metrics dashboard provides standard visualizations of monthly industry trends. When This project is on pause for now. Where https://tlcanalytics.shinyapps.io/dash_test/ Who Nikita Voevodin is the creator and maintainer of the project. Other The project was put on pause due to a lack of data updates. It should be switched to monthly updates (parallel to the raw trips publishing timeline) and reinstated. 8.5 Raw Trip Records publishing What Every six months, TLC aims to publicly release the previous six months of raw trip record data on our website and Open Data. Process: Ticket to Lana to create monthly files Review with Chair Send Konstantin links for him to stage (they will have predictable names based on month and industry) Ticket to Lana to load files to AWS Send links to Alex Finkel at DoITT to post to Open Data When It is a recurring project. It is updated on the first weeks of September and March, Bi-Annually. Where User Guide: https://www1.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf Yellow Dictionary: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf Green Dictionary: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf FHV Dictionary: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf High Volume Dictionary: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf Who Point: Nikita Voevodin runs the updates. Support: IT/ Web Konstantin Onishchenko, PR Alan Fromberg, Rebecca Harshbarger Support IT/Data: Lana Goldenberg, IT/ Web: Konstantin Onishchenko, PR: Alan Fromberg, Rebecca Harshbarger Other Publish raw trip records monthly on a two-month delay. Reason for 2-month delay: trad fhv bases submit their data with varying delay (4-6 weeks). For reference, the HVFHV delay is 2-3 weeks, yellow and green: 2 weeks. I do not recommend releasing the data as it comes or on different schedules, as the process is very time-consuming. As of now, we release bi-annually. Releasing monthly is x6 the workload. Releasing as it comes is x24 the workload. Additionally, releasing on a 2-month delay schedule would allow us to catch submission errors and ensure data integrity. 8.6 Data Tasks Spreadsheet There are many more tasks that we handle. Some of them are listed in the Recurring_Tasks Document located at: I:\\COF\\COF\\_DA&amp;E_\\Nikita\\Reports\\Task_spreadsheet Recurring tasks ex This work is licensed under a Creative Commons Attribution 4.0 International License. "],["interesting-projects-up-for-grabs.html", "Chapter 9 Interesting Projects Up for Grabs 9.1 TLC Open Data Coordinator 9.2 TLC Data Blog 9.3 TLC Data Meetup", " Chapter 9 Interesting Projects Up for Grabs We always had and should aim at having exciting things to do. Those things do not necessarily have to be task or policy-related. They can be the initiatives that you think might benefit you, the team, or TLC in the long run. The interpretation of what helps the team or the agency can be broad. A regular potluck can build team spirit and encourage cooperation. As a result, overall productivity might improve. Here are some of the existing things that you can take on and become a point person on it: 9.1 TLC Open Data Coordinator Local law requires us to comply with open data requirements that necessitate we move all machine-readable data on our website to Open data. I am not sure if we got a 100% committed point on thisTed Metz from Policy and Melissa Cintron from Legal cover the role now. The role will require coordinating our data uploads to the open data city server, answering follow-up questions from outside the agency, and fixing data problems using programming skills. It is a good chunk of responsibility and an additional title on your resume. It should allow you to build valuable connections across the city agencies and access city open data-related events. 9.2 TLC Data Blog To develop talent internally at TLC and increase the publics knowledge of taxi affairs, the Policy and External Affairs team contributes to the TLC medium data blog. Anyone can post, and all postings must go through an approval process with the Assistant Commissioner of Data &amp; Tech and the current deputy commissioner of Public Affairs. The blog can be accessed through our website as well at: https://medium.com/@NYCTLC Your responsibilities will include: - Aggregating ideas for potential posts. - Tracking progress from the content creators. - Liaising with HR to get them published and promoted. 9.3 TLC Data Meetup This project aims to build cooperation across the agency, develop talent, and let leadership know what we are capable of. The role will require encouraging people to participate, accumulating ideas, coordinating schedules, maintaining the existing data meetup app. This role is both challenging and rewarding. It is a recurring event that assembles data, programming, and transportation enthusiasts across the agency to present exciting things that they work on and show off their technical skills. "],["useful-code-snippets-tricks.html", "Chapter 10 Useful Code Snippets &amp; Tricks 10.1 R Package Installer 10.2 FST data format", " Chapter 10 Useful Code Snippets &amp; Tricks This section includes helpful code snippets, shortcuts, tips, and tricks that will make your coding life a little bit easier. 10.1 R Package Installer It can be annoying to install package by package when you first setting up your environment. Here is the function that I created that installs and loads a bunch of initial packages for you. #install procedures for relevant R packages--------------------------------------------------------------------- #author: Nikita Voevedin #notes: this script is revisited for updates every month, note that you may have to install a package seperately #COPY THIS SCRIPT AND RUN IN RSTUDIO #######################script that automatically installs all libraries in a vector############################## installer &lt;- function(x){ for( i in x ){ if( !require( i , character.only = TRUE ) ){ # If not loading - install install.packages( i , dependencies = TRUE ) # Load require(i, character.only = TRUE ) } } } ######################################################################################################################### installer( c(&quot;ggplot2&quot; , &quot;reshape2&quot; , &quot;data.table&quot;, &#39;shiny&#39;, &#39;shinydashboard&#39;, &#39;fasttime&#39;, &#39;lubridate&#39;, &#39;dplyr&#39;, &#39;httr&#39;, &#39;jsonlite&#39;, &#39;RCurl&#39;, &#39;pbapply&#39;, &#39;tidyverse&#39;, &#39;openxlsx&#39;, &#39;readxl&#39;, &#39;reshape&#39;, &#39;tidyr&#39;, &#39;plyr&#39;, &#39;ggmap&#39;, &#39;leaflet&#39;, &#39;raster&#39;, &#39;sf&#39;, &#39;rgdal&#39;, &#39;leaflet.extras&#39;, &#39;remotes&#39;, &#39;stringi&#39;, &#39;shiny&#39;, &#39;rgeos&#39;, &#39;rmapshaper&#39;, &#39;sp&#39;, &#39;echarts4r&#39;, &#39;shinydashboardPlus&#39;, &#39;shinycssloaders&#39;, &#39;DT&#39;, &#39;mapview&#39;,&#39;parallel&#39;) ) ########################################################################################################################### 10.2 FST data format Although it is ok to use csv and for 90% of our tasks we will be using csvs, if the amount of data is too big and you need faster loading speeds, you can use fst files. Fst files leverage SSD drive speeds and were created by data teams at facebook to speed up read and write times. Since they are native to R, they are easier to read in R. Make sure you have the fst packge installed with the install.packages(fst). Below you can see how you read in a file: library(fst) library(data.table) #working directory setwd(&quot;I:/COF/COF/_M3trics2/records/med&quot;) #Extracting one day of medallion data data &lt;- read.fst(list.files(pattern = &quot;2017-01-01&quot;), as.data.table = T) "],["dictionary-and-terminology.html", "Chapter 11 Dictionary and Terminology", " Chapter 11 Dictionary and Terminology TLC hosts tons of data points, and it is not always clear whats what. You will spend a substantial amount of time figuring out different codes, terms, and abbreviations. This section should make it a little easier and serve as an introductory data dictionary for you. Table 11.1: Data Dictionary Term Description General Camis Id Unique identifier in Camis_Entity table Tpep Yellow Taxis Lpep Green Cabs HVFHV High Volume companies (Uber, Lyft) FHV (Traditional) Liveries, Black cars, Limos Medallions Yellow Taxis SHL Green Cabs Company Codes HV0002 JUNO HV0003 UBER HV0004 VIA HV0005 LYFT "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
